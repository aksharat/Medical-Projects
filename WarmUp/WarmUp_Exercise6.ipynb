{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksharat/Medical-Projects/blob/main/WarmUp/WarmUp_Exercise6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkE1Chm-IMmN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
        "from torchvision import transforms, datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/segmentation02.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvNg0ikFKOHU",
        "outputId": "9d7b3b6a-93c0-4fa6-81dd-a62526d01a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-23 18:17:03--  http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/segmentation02.zip\n",
            "Resolving imgcom.jsrt.or.jp (imgcom.jsrt.or.jp)... 158.199.228.161\n",
            "Connecting to imgcom.jsrt.or.jp (imgcom.jsrt.or.jp)|158.199.228.161|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23368008 (22M) [application/zip]\n",
            "Saving to: ‘segmentation02.zip’\n",
            "\n",
            "segmentation02.zip  100%[===================>]  22.29M   974KB/s    in 31s     \n",
            "\n",
            "2023-09-23 18:17:34 (737 KB/s) - ‘segmentation02.zip’ saved [23368008/23368008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/segmentation02.zip"
      ],
      "metadata": {
        "id": "n69irsK8KrqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "model = maskrcnn_resnet50_fpn(weights=weights)\n",
        "# transforms = weights.transforms()"
      ],
      "metadata": {
        "id": "Zfs27OYZRgJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def detect_all_classes_bounding_boxes_and_save(label_dir, output_dir):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through all label images in the label_dir\n",
        "    for filename in os.listdir(label_dir):\n",
        "        if filename.endswith(\"_label.png\"):\n",
        "            label_image_path = os.path.join(label_dir, filename)\n",
        "\n",
        "            # Define pixel value ranges for each class\n",
        "            class_pixel_ranges = [\n",
        "                ((255, 255, 255), (255, 255, 255), (255, 255, 255)),  # Lung field\n",
        "                ((85, 85, 85), (85, 85, 85), (85, 85, 85)),  # Heart region\n",
        "                ((170, 170, 170), (170, 170, 170), (170, 170, 170)),  # Another Lung field\n",
        "                ((0, 0, 0), (0, 0, 0), (0, 0, 0))  # Outside the body\n",
        "            ]\n",
        "\n",
        "            # Load the label image (teacher image)\n",
        "            label_image = cv2.imread(label_image_path)\n",
        "\n",
        "            # Initialize a dictionary to store bounding boxes for each class\n",
        "            bounding_boxes = {i: [] for i in range(len(class_pixel_ranges))}\n",
        "\n",
        "            # Create masks for each class and find contours\n",
        "            for class_idx, pixel_range in enumerate(class_pixel_ranges):\n",
        "                masks = [cv2.inRange(label_image, np.array(range), np.array(range)) for range in pixel_range]\n",
        "                for mask in masks:\n",
        "                    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                    for contour in contours:\n",
        "                        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "                        # Check if the bounding box is valid\n",
        "                        if x >= 0 and y >= 0 and w > 0 and h > 0:\n",
        "                            bounding_boxes[class_idx].append({\"x\": x, \"y\": y, \"width\": w, \"height\": h})\n",
        "\n",
        "            # Combine bounding boxes for \"chest area\" (excluding outside and heart)\n",
        "            chest_area_bounding_boxes = []\n",
        "            for i in range(len(class_pixel_ranges)):\n",
        "                if i not in [0, 3, 1]:\n",
        "                    chest_area_bounding_boxes += bounding_boxes[i]\n",
        "\n",
        "            # Save bounding box information to a JSON file\n",
        "            case_name = os.path.splitext(filename)[0]  # Get the case name without extension\n",
        "            output_json_path = os.path.join(output_dir, f\"{case_name}.json\")\n",
        "            bounding_box_info = {\n",
        "                \"LungField\": bounding_boxes[0],\n",
        "                \"HeartRegion\": bounding_boxes[1],\n",
        "                \"AnotherLungField\": bounding_boxes[2],\n",
        "                \"OutsideTheBody\": bounding_boxes[3],\n",
        "                \"ChestArea\": chest_area_bounding_boxes\n",
        "            }\n",
        "\n",
        "            with open(output_json_path, 'w') as json_file:\n",
        "                json.dump(bounding_box_info, json_file, indent=2)  # Use indent for pretty formatting\n",
        "\n",
        "            print(f\"Saved bounding box information for {case_name} to {output_json_path}\")\n",
        "\n",
        "# Example usage:\n",
        "train_label_dir = '/content/segmentation02/segmentation/label_train'\n",
        "test_label_dir = '/content/segmentation02/segmentation/label_test'\n",
        "train_output_dir = '/content/train_bbox_info'  # Modify this as needed\n",
        "test_output_dir = '/content/test_bbox_info'  # Modify this as needed\n",
        "\n",
        "detect_all_classes_bounding_boxes_and_save(train_label_dir, train_output_dir)\n",
        "detect_all_classes_bounding_boxes_and_save(test_label_dir, test_output_dir)\n"
      ],
      "metadata": {
        "id": "OorFuYv4E__6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_dir, json_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.json_dir = json_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = os.listdir(image_dir)\n",
        "        self.json_files = os.listdir(json_dir)\n",
        "        self.mask_paths = os.listdir(mask_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.image_paths[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_paths[idx].replace('.bmp', '_label.png'))\n",
        "        json_file = os.path.join(self.json_dir, self.mask_paths[idx].replace('.png', '.json'))\n",
        "\n",
        "        # Load image, mask, and JSON file\n",
        "        image = cv2.imread(image_path)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Load JSON file and check for issues\n",
        "        with open(json_file, 'r') as f:\n",
        "            bbox_info = json.load(f)\n",
        "\n",
        "\n",
        "        # Convert numpy arrays to PIL Images\n",
        "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        mask = Image.fromarray(mask)\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)  # Apply the same transformation to the mask\n",
        "\n",
        "        return image, mask, bbox_info\n",
        "\n",
        "\n",
        "def check_bbox_issues(image, mask, bbox_info):\n",
        "    issues = []\n",
        "\n",
        "    for category, bboxes in bbox_info.items():\n",
        "        for bbox in bboxes:\n",
        "            x = bbox[\"x\"]\n",
        "            y = bbox[\"y\"]\n",
        "            width = bbox[\"width\"]\n",
        "            height = bbox[\"height\"]\n",
        "\n",
        "            if x < 0 or y < 0 or x + width > image.shape[1] or y + height > image.shape[0]:\n",
        "                issues.append(f\"Invalid bbox in category '{category}': {bbox}\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g6MsQ49LWw4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "train_image_dir = '/content/segmentation02/segmentation/org_train'\n",
        "train_json_dir = '/content/train_bbox_info'\n",
        "train_mask_dir = '/content/segmentation02/segmentation/label_train'\n",
        "\n",
        "# Define image transformations if needed (e.g., resizing, normalization)\n",
        "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
        "\n",
        "# Create a custom dataset instance for training\n",
        "train_dataset = CustomDataset(train_image_dir, train_json_dir, train_mask_dir, transform)\n"
      ],
      "metadata": {
        "id": "3gFCXcohbNbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "test_image_dir = '/content/segmentation02/segmentation/org_test'\n",
        "test_json_dir = '/content/test_bbox_info'\n",
        "test_mask_dir = '/content/segmentation02/segmentation/label_test'\n",
        "\n",
        "\n",
        "# Create a custom dataset instance for training\n",
        "test_dataset = CustomDataset(test_image_dir, test_json_dir, test_mask_dir, transform)\n"
      ],
      "metadata": {
        "id": "hyj5uYZFbgff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Example usage:\n",
        "train_batch_size = 1  # You can adjust this batch size as needed\n",
        "test_batch_size = 1   # You can adjust this batch size as needed\n",
        "\n",
        "# Create DataLoader instances with the custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Df35_aUAbtIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access properties of the train_loader\n",
        "print(f\"Train DataLoader Properties:\")\n",
        "print(f\"Batch Size: {train_loader.batch_size}\")\n",
        "print(f\"Number of Workers: {train_loader.num_workers}\")\n",
        "print(f\"Drop Last Batch: {train_loader.drop_last}\")\n",
        "print(f\"Dataset Length: {len(train_loader.dataset)}\")\n",
        "\n",
        "# Access properties of the test_loader\n",
        "print(f\"\\nTest DataLoader Properties:\")\n",
        "print(f\"Batch Size: {test_loader.batch_size}\")\n",
        "print(f\"Number of Workers: {test_loader.num_workers}\")\n",
        "print(f\"Drop Last Batch: {test_loader.drop_last}\")\n",
        "print(f\"Dataset Length: {len(test_loader.dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oydTTA2d_zO",
        "outputId": "61c38a4f-787d-4fda-d8b1-87f1bdef9f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataLoader Properties:\n",
            "Batch Size: 1\n",
            "Number of Workers: 0\n",
            "Drop Last Batch: False\n",
            "Dataset Length: 199\n",
            "\n",
            "Test DataLoader Properties:\n",
            "Batch Size: 1\n",
            "Number of Workers: 0\n",
            "Drop Last Batch: False\n",
            "Dataset Length: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "# Initialize the Mask R-CNN model with ResNet-50 backbone\n",
        "model = maskrcnn_resnet50_fpn(weights=weights)  # You can specify pretrained=True if you want pre-trained weights\n",
        "\n",
        "# Modify the number of classes in the model\n",
        "num_classes = 5  # Replace with the number of classes in your dataset\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "# Set the device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "cG_294b5kEj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define loss function and optimizer (replace with your own)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Replace with the number of training epochs you want\n",
        "train_losses = []\n",
        "test_losses = []"
      ],
      "metadata": {
        "id": "JW7bh-Hak9dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "sT2uNmRnm_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already created your train_loader\n",
        "print(train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKmtud54pAxO",
        "outputId": "54d71f0b-1279-487f-ad0b-4a64d3c460e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7d37ae1330d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, targets,bbox) in enumerate(train_loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += losses.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "m9ao8apblivC",
        "outputId": "e5b51c0c-e006-41dc-a364-f9b766730a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b4908fd26d51>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b4908fd26d51>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(train_loader)\n",
        "print(f\"Total number of batches in train_loader: {num_batches}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZG1YxtKloUW",
        "outputId": "041d883c-9db7-43be-c5be-7223f03ddeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of batches in train_loader: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from PIL import Image\n",
        "\n",
        "# Directory containing the .bmp images\n",
        "image_dir = \"/content/segmentation02/segmentation/org_train_s/\"\n",
        "\n",
        "# List all the .bmp image files in the directory\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith(\".bmp\")]\n",
        "\n",
        "# Define the transformation to apply to each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert image to a PyTorch tensor\n",
        "])\n",
        "\n",
        "# Load and preprocess each image\n",
        "images = []\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    image = Image.open(image_path).convert(\"RGB\")  # Open the .bmp image and convert to RGB mode\n",
        "    image = transform(image)  # Apply the defined transformation\n",
        "    images.append(image)\n",
        "\n",
        "# Create the Faster R-CNN model with default weights\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
        "model = model.eval()\n",
        "\n",
        "# Perform inference on the preprocessed images\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "\n",
        "# outputs now contains the detection results for each input image\n",
        "print(outputs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es3tkf9-spuG",
        "outputId": "e04f50f5-5e59-4ac0-c9c3-b1d60759e0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[ 10.5409,  18.1177, 249.7619, 248.6298],\n",
            "        [ 16.5886,  19.8839, 220.0952, 242.7390],\n",
            "        [  0.3504,  16.9393, 255.5071, 241.3622],\n",
            "        [135.5676,  24.6512, 245.1338, 224.5948],\n",
            "        [  5.0574,  26.8288, 140.5573, 232.9227],\n",
            "        [  8.3246,  21.7068, 124.5667, 215.0065],\n",
            "        [ 81.2057,  26.9080, 214.6474, 203.7318],\n",
            "        [131.5770,  17.5968, 250.8983, 246.8999],\n",
            "        [  1.5200,  11.9907, 253.7998, 253.0857],\n",
            "        [ 80.0196,  27.9006, 196.3253, 198.9382],\n",
            "        [  0.0000,  19.2021, 255.2403, 240.8332],\n",
            "        [161.8391,  16.6125, 251.6003, 243.8387],\n",
            "        [121.2233,  25.4828, 252.5641, 248.5527],\n",
            "        [107.5567,  22.0460, 248.9638, 243.1363],\n",
            "        [  8.3708,  18.1344, 126.0205, 217.8614],\n",
            "        [ 18.0580,  14.0012, 121.3181, 223.4219],\n",
            "        [ 24.8805,  23.9808, 226.5685, 232.8967]]), 'labels': tensor([ 1, 17, 63,  1,  1, 63,  1, 63, 62, 17, 65, 65, 62, 32, 65, 62, 18]), 'scores': tensor([0.5965, 0.4321, 0.3697, 0.3426, 0.2162, 0.2138, 0.2105, 0.1516, 0.1216,\n",
            "        0.1192, 0.1187, 0.1107, 0.0919, 0.0807, 0.0754, 0.0654, 0.0646])}, {'boxes': tensor([[ 27.1699,   0.0000, 243.9868, 252.1792],\n",
            "        [  9.2965,   5.4262, 245.6849, 251.9470],\n",
            "        [  6.1582,  14.8747, 138.7416, 242.6127],\n",
            "        [142.5440,   0.0000, 246.3813, 252.3040],\n",
            "        [  7.2267,  10.4185, 134.6743, 237.8028],\n",
            "        [ 12.5745,   5.5327, 126.5453, 240.7115],\n",
            "        [145.0175,   6.3443, 245.9132, 256.0000],\n",
            "        [  5.6450,   9.7976, 244.9042, 252.9381],\n",
            "        [144.4249,   0.0000, 247.6586, 248.6032],\n",
            "        [ 92.3990,   7.6504, 191.3050, 220.3026],\n",
            "        [123.0769,   9.7258, 240.5243, 245.2262],\n",
            "        [  7.4597,   3.6978, 154.2283, 248.1363],\n",
            "        [ 75.1699,   6.2897, 201.9578, 249.4193]]), 'labels': tensor([ 1, 65,  1,  1, 65, 32, 65, 63, 63,  1, 32, 62, 32]), 'scores': tensor([0.6524, 0.4498, 0.3422, 0.2078, 0.1941, 0.1673, 0.1316, 0.0806, 0.0796,\n",
            "        0.0790, 0.0777, 0.0636, 0.0594])}, {'boxes': tensor([[ 27.5871,  28.3377, 234.7068, 246.2046],\n",
            "        [  4.4399,  17.3949, 253.4604, 256.0000],\n",
            "        [  6.0861,  19.9889, 113.7623, 224.2508],\n",
            "        [  0.0000,  10.4406, 251.3315, 255.2240],\n",
            "        [  0.5283,   0.0000, 249.8457, 256.0000],\n",
            "        [  4.1494,  24.8806, 113.0441, 241.5853],\n",
            "        [120.8627,   5.4748, 249.4577, 251.7219],\n",
            "        [  6.9223,  29.4909, 140.0949, 246.1841],\n",
            "        [ 12.1950, 163.8716, 240.7074, 246.2529],\n",
            "        [  0.0000,  12.6766, 255.9916, 251.4624],\n",
            "        [  9.6539, 157.4639, 241.1303, 251.5350],\n",
            "        [ 22.4467,  23.5849, 239.4969, 244.7309],\n",
            "        [ 83.4001,  15.7341, 184.9838, 194.6888],\n",
            "        [ 13.5843, 168.3230, 236.7142, 250.4862],\n",
            "        [121.5116,   8.0460, 249.1193, 251.4230],\n",
            "        [ 93.4388,  23.2419, 237.0422, 196.7327],\n",
            "        [  6.1562,  23.3708, 145.6609, 237.8076],\n",
            "        [121.3467,   9.7746, 251.9516, 247.9941]]), 'labels': tensor([17, 65, 65, 63,  1, 63, 65,  1,  1, 62, 17, 18, 17, 62, 63,  1, 62, 62]), 'scores': tensor([0.6181, 0.5214, 0.3662, 0.3276, 0.3103, 0.1959, 0.1936, 0.1715, 0.1322,\n",
            "        0.1128, 0.1123, 0.1038, 0.0902, 0.0897, 0.0872, 0.0865, 0.0774, 0.0569])}, {'boxes': tensor([[1.2850e-01, 1.9923e+01, 2.5600e+02, 2.4325e+02],\n",
            "        [5.6225e+00, 9.5695e+00, 2.2392e+02, 2.4337e+02],\n",
            "        [8.6613e+00, 9.5882e+00, 1.4805e+02, 2.2254e+02],\n",
            "        [2.0469e+00, 1.3214e+01, 2.5434e+02, 2.3760e+02],\n",
            "        [1.2291e+02, 1.0637e+01, 2.5065e+02, 2.4535e+02],\n",
            "        [2.4857e+01, 0.0000e+00, 2.0446e+02, 2.4523e+02],\n",
            "        [1.2262e+02, 1.4042e+01, 2.4866e+02, 2.4639e+02],\n",
            "        [1.2388e+02, 1.4017e+01, 2.5061e+02, 2.4223e+02],\n",
            "        [0.0000e+00, 1.6895e+01, 2.5471e+02, 2.4717e+02],\n",
            "        [9.4630e+00, 1.2721e+01, 1.3806e+02, 2.4929e+02],\n",
            "        [1.0190e+02, 1.5382e+01, 2.2218e+02, 2.2623e+02],\n",
            "        [1.2184e+02, 8.4105e+00, 2.5401e+02, 2.2078e+02],\n",
            "        [5.6772e+00, 4.4427e+01, 2.4908e+02, 2.5559e+02],\n",
            "        [6.4689e+00, 0.0000e+00, 1.2027e+02, 2.4941e+02]]), 'labels': tensor([63,  1, 65, 62, 62, 17, 86, 63, 65, 63,  1, 65, 86, 62]), 'scores': tensor([0.4502, 0.4029, 0.3208, 0.2831, 0.2418, 0.2166, 0.2132, 0.1895, 0.1617,\n",
            "        0.1297, 0.0978, 0.0758, 0.0688, 0.0579])}, {'boxes': tensor([[  7.3752,  25.0131, 238.9969, 239.5038],\n",
            "        [111.8557,   7.4979, 250.9187, 245.6166],\n",
            "        [  6.1287,  57.2229, 253.3161, 254.0507],\n",
            "        [  1.4009,  14.0443, 256.0000, 251.6677],\n",
            "        [  2.4236,  88.2252,  40.0825, 248.7156],\n",
            "        [ 29.9246,  18.5029, 212.5506, 250.4531],\n",
            "        [ 24.3281,  10.3062, 246.5106, 250.0992],\n",
            "        [216.8455,  37.0148, 253.7590, 244.3207],\n",
            "        [ 25.5564,  18.1359, 219.1888, 251.1999],\n",
            "        [  3.6913,  13.9609, 141.2021, 252.5943],\n",
            "        [122.2624,  16.2157, 250.8852, 232.0503],\n",
            "        [ 24.1138,  48.3887, 118.0728, 250.4948],\n",
            "        [  5.4073,   9.2650, 142.2445, 232.2880],\n",
            "        [  0.0000,   9.8462, 256.0000, 253.1494],\n",
            "        [  5.2923,  16.0263, 142.5100, 229.7882],\n",
            "        [116.2722,  30.2584, 212.8256, 240.9035]]), 'labels': tensor([ 1, 62, 62, 63, 62, 17, 86, 62, 18, 62, 63,  1, 65, 65, 63,  1]), 'scores': tensor([0.7304, 0.5518, 0.4505, 0.2705, 0.1905, 0.1733, 0.1599, 0.1331, 0.0888,\n",
            "        0.0802, 0.0799, 0.0737, 0.0644, 0.0607, 0.0581, 0.0552])}, {'boxes': tensor([[  9.1296,  19.0207, 240.1543, 241.2558],\n",
            "        [ 24.8553,  17.5328, 230.0750, 239.8016],\n",
            "        [  0.0000,   1.6889, 248.7310, 250.0238],\n",
            "        [ 10.9078,  23.2071, 124.9507, 238.2060],\n",
            "        [126.2252,  18.1396, 228.4177, 228.1661],\n",
            "        [  0.0000,  12.3768, 254.4674, 247.2568],\n",
            "        [121.8725,  24.2638, 223.2991, 243.6711],\n",
            "        [  7.3920,  14.9280, 153.1016, 191.0390],\n",
            "        [  0.0000,   8.3790, 256.0000, 254.7858],\n",
            "        [ 83.8899,  23.3513, 199.4810, 209.1550],\n",
            "        [ 16.1083,  17.0871, 238.8731, 241.2438]]), 'labels': tensor([ 1, 17, 65,  1,  1, 63, 32, 65, 62,  1, 18]), 'scores': tensor([0.5226, 0.4096, 0.3149, 0.2817, 0.2527, 0.2025, 0.0941, 0.0802, 0.0730,\n",
            "        0.0581, 0.0507])}, {'boxes': tensor([[ 20.0733,   8.8947, 221.9399, 251.9274],\n",
            "        [  0.6583,  15.9026, 256.0000, 256.0000],\n",
            "        [125.4294,   9.6604, 251.3544, 255.9261],\n",
            "        [ 74.7906,  12.9980, 249.3741, 254.9136],\n",
            "        [  0.0000,   8.8018, 256.0000, 253.4778],\n",
            "        [  1.5632,  20.7825, 163.0254, 247.9719],\n",
            "        [  9.1522,  15.7732, 126.8710, 208.7427],\n",
            "        [124.5925,  12.8271, 252.2851, 256.0000],\n",
            "        [ 18.2499,   6.5162, 227.4635, 247.7836],\n",
            "        [119.3521,  15.9081, 220.3899, 220.0069],\n",
            "        [ 62.7843,  12.2226, 201.2869, 187.5524],\n",
            "        [  3.0572,  22.1273,  40.7383, 244.0076]]), 'labels': tensor([17, 63, 63,  1, 62,  1, 63, 62, 18,  1,  1, 63]), 'scores': tensor([0.5486, 0.5239, 0.4558, 0.4211, 0.2765, 0.2519, 0.1692, 0.1405, 0.0946,\n",
            "        0.0807, 0.0685, 0.0684])}, {'boxes': tensor([[  4.7641,  10.5953, 256.0000, 244.3556],\n",
            "        [  0.0000,  14.3906, 256.0000, 249.8067],\n",
            "        [ 15.5061,   7.8099, 253.0626, 255.4872],\n",
            "        [ 32.9246, 163.2849, 228.0687, 249.7813],\n",
            "        [ 30.1929,   1.4474, 226.6777, 248.1554],\n",
            "        [123.9267,   7.1739, 250.2513, 217.6381],\n",
            "        [  0.0000,  11.8827, 256.0000, 245.3532],\n",
            "        [ 11.4640,  26.2718, 127.9965, 230.4767],\n",
            "        [124.0639,  10.7813, 250.2524, 221.0632],\n",
            "        [ 30.5860,  22.4867, 142.5108, 190.4558],\n",
            "        [  8.2159,  16.1888, 128.7000, 208.8922],\n",
            "        [ 32.4273,   9.2195, 223.2234, 240.4676],\n",
            "        [  6.8331,  13.6723, 127.8703, 209.8240],\n",
            "        [164.6046,  13.6823, 252.4201, 249.2539],\n",
            "        [135.4010, 180.3968, 223.6063, 250.5000],\n",
            "        [ 16.5985,  56.2404, 238.3719, 251.2601],\n",
            "        [120.9954,   6.9808, 251.1458, 224.2103]]), 'labels': tensor([ 1, 63, 62,  1, 17,  1, 65,  1, 63, 32, 63, 32, 65, 62, 37, 86, 65]), 'scores': tensor([0.5338, 0.3770, 0.2438, 0.1918, 0.1821, 0.1317, 0.1152, 0.0909, 0.0882,\n",
            "        0.0826, 0.0697, 0.0652, 0.0608, 0.0572, 0.0540, 0.0530, 0.0514])}, {'boxes': tensor([[ 14.8731,   0.0000, 236.9945, 243.5224],\n",
            "        [  1.2640,  13.2032, 255.9337, 237.1851],\n",
            "        [  5.5024,   7.6084, 204.4496, 243.8018],\n",
            "        [  1.5493,   7.8468, 256.0000, 234.7387],\n",
            "        [ 23.1429,   4.5448, 146.5099, 187.7552],\n",
            "        [ 79.1756,   5.1891, 254.6350, 248.7259],\n",
            "        [  7.8886,   6.5444, 124.8974, 248.8952],\n",
            "        [125.7778,   8.9556, 254.9085, 224.9115],\n",
            "        [122.6998,   4.7307, 252.0060, 226.4420],\n",
            "        [  4.9589,  14.6923, 145.5588, 247.7199],\n",
            "        [  7.7953,   5.5749, 127.8516, 246.5484],\n",
            "        [  2.4968,  24.6277,  58.6734, 206.8555],\n",
            "        [104.8757,  11.0210, 220.9409, 204.1849],\n",
            "        [197.4256,  12.0692, 254.0535, 233.6562],\n",
            "        [  2.9814,  19.0797,  59.6378, 208.0231]]), 'labels': tensor([17, 63,  1, 65,  1, 62, 63, 63, 65, 62, 65, 63,  1, 63, 65]), 'scores': tensor([0.5018, 0.4997, 0.4862, 0.2125, 0.2080, 0.2068, 0.1986, 0.1514, 0.0995,\n",
            "        0.0854, 0.0720, 0.0693, 0.0660, 0.0613, 0.0588])}, {'boxes': tensor([[  0.0000,  18.9642, 256.0000, 245.0915],\n",
            "        [  4.3258,  13.8711, 254.6898, 236.1229],\n",
            "        [  4.8451,  10.5023, 146.2423, 212.4878],\n",
            "        [  0.0000,  13.2685, 256.0000, 240.5503],\n",
            "        [ 14.4196,   0.0000, 239.2023, 247.0878],\n",
            "        [  0.0000,   9.4752, 256.0000, 246.1578],\n",
            "        [  3.7292,  15.8150, 147.1125, 209.0659],\n",
            "        [120.8630,  15.4733, 248.2544, 225.5770]]), 'labels': tensor([63,  1, 65, 65, 32, 62, 63,  1]), 'scores': tensor([0.3790, 0.3324, 0.3302, 0.3100, 0.2191, 0.1848, 0.1360, 0.0843])}, {'boxes': tensor([[ 30.9130,  17.3330, 233.5598, 238.3873],\n",
            "        [  7.4885,   6.8758, 246.8166, 249.2068],\n",
            "        [  1.4336,   0.0000, 253.6411, 247.8913],\n",
            "        [  2.1858,  19.9679, 256.0000, 249.6137],\n",
            "        [123.0838,  16.8979, 249.7200, 236.8185],\n",
            "        [  1.9169,  13.1502, 255.7954, 252.5314],\n",
            "        [  3.5188,  16.2501, 138.9089, 238.3676],\n",
            "        [ 66.0085,  24.0418, 211.9781, 188.7864],\n",
            "        [131.8596,   7.5981, 254.5625, 247.4607],\n",
            "        [131.3047,   2.5498, 252.7717, 245.5150],\n",
            "        [  8.1833,   2.5136, 247.2522, 246.6563]]), 'labels': tensor([17,  1, 62, 63,  1, 65,  1,  1, 62, 63, 86]), 'scores': tensor([0.3903, 0.3827, 0.2284, 0.2072, 0.1378, 0.1370, 0.1128, 0.1120, 0.1014,\n",
            "        0.0739, 0.0719])}, {'boxes': tensor([[2.7701e+01, 2.4589e+01, 1.9134e+02, 2.4705e+02],\n",
            "        [0.0000e+00, 1.1077e+01, 2.5501e+02, 2.3750e+02],\n",
            "        [1.2687e+02, 7.8705e+00, 2.5429e+02, 2.3448e+02],\n",
            "        [1.0063e+02, 5.5168e+01, 1.8399e+02, 1.9627e+02],\n",
            "        [3.8278e+00, 6.9093e+00, 2.5060e+02, 2.5507e+02],\n",
            "        [0.0000e+00, 5.9678e-02, 2.5600e+02, 2.5307e+02],\n",
            "        [2.3408e+01, 1.9799e+01, 2.0892e+02, 2.4215e+02],\n",
            "        [1.2709e+02, 1.0896e+01, 2.5322e+02, 2.3475e+02],\n",
            "        [6.5670e+00, 4.0592e+00, 1.3556e+02, 2.2820e+02],\n",
            "        [9.7641e+01, 2.7003e+01, 2.4597e+02, 2.4212e+02],\n",
            "        [6.8823e+00, 3.7037e+00, 2.5094e+02, 2.5120e+02],\n",
            "        [3.2149e+01, 2.1559e+01, 2.1308e+02, 2.3965e+02],\n",
            "        [2.2835e+01, 1.7477e+02, 2.3054e+02, 2.5166e+02],\n",
            "        [4.8084e+00, 6.8565e+00, 1.3509e+02, 2.5238e+02],\n",
            "        [1.3120e+02, 6.0227e+01, 1.6564e+02, 1.4650e+02],\n",
            "        [1.3649e+02, 4.9011e+00, 2.5337e+02, 2.4063e+02],\n",
            "        [9.9400e+01, 5.2429e+01, 1.7404e+02, 2.0807e+02],\n",
            "        [1.0808e+02, 3.7975e+01, 2.3548e+02, 2.3402e+02],\n",
            "        [9.9453e-02, 0.0000e+00, 2.5600e+02, 2.5600e+02],\n",
            "        [1.4827e+02, 1.8527e+02, 2.3760e+02, 2.5134e+02],\n",
            "        [4.9882e-01, 0.0000e+00, 2.5411e+02, 2.5600e+02]]), 'labels': tensor([17, 63, 62,  1, 65, 62,  1, 63, 65, 17, 86, 18,  1, 63,  1, 65, 17,  1,\n",
            "        72,  1, 33]), 'scores': tensor([0.4092, 0.3936, 0.3006, 0.2705, 0.2398, 0.2347, 0.1912, 0.1589, 0.1387,\n",
            "        0.1300, 0.1194, 0.1106, 0.1022, 0.0998, 0.0930, 0.0811, 0.0778, 0.0695,\n",
            "        0.0663, 0.0620, 0.0511])}, {'boxes': tensor([[  5.5927,  31.4697, 256.0000, 234.4740],\n",
            "        [132.0352,  19.6169, 253.0161, 254.4972],\n",
            "        [  0.0000,  28.8433, 256.0000, 254.2055],\n",
            "        [  0.0000,  23.5261, 256.0000, 256.0000],\n",
            "        [  6.8257,  23.2642, 138.8009, 247.3496],\n",
            "        [ 96.4211,  21.0635, 254.8226, 250.7618],\n",
            "        [ 27.6919,  20.9931, 200.7600, 245.3858],\n",
            "        [  5.3948,  20.1804, 130.4109, 247.5431],\n",
            "        [133.8975,  20.5506, 255.2868, 221.6201],\n",
            "        [  5.0220,  20.0998, 133.3598, 246.7783],\n",
            "        [128.4949,  33.3069, 237.7903, 214.9922],\n",
            "        [  3.0091,  21.6047, 186.0526, 248.5125]]), 'labels': tensor([ 1, 63, 63, 65,  1, 62, 17, 63, 65, 65,  1, 62]), 'scores': tensor([0.4799, 0.3999, 0.3493, 0.2811, 0.2506, 0.2168, 0.1681, 0.1505, 0.1425,\n",
            "        0.0921, 0.0705, 0.0684])}, {'boxes': tensor([[  3.7427,  12.9937, 162.5454, 251.2907],\n",
            "        [  3.7538,  14.5720, 253.1481, 243.3126],\n",
            "        [102.6715,   8.3633, 247.2062, 251.4898],\n",
            "        [  4.6469,   0.6246, 247.3214, 254.7798],\n",
            "        [  0.6870,   7.7237, 253.8454, 253.5671],\n",
            "        [ 29.7991,  18.9229, 233.7858, 250.8432],\n",
            "        [154.5654,  12.6933, 248.9256, 248.2361],\n",
            "        [ 27.8644,   9.5972, 213.0444, 250.2374],\n",
            "        [208.4196,  24.4101, 252.0841, 249.5453],\n",
            "        [ 22.5583,  75.4162, 111.7435, 247.3364],\n",
            "        [121.6607,   3.9330, 249.8835, 196.6773]]), 'labels': tensor([ 1, 63,  1, 62, 65, 18, 63, 17, 63,  1, 65]), 'scores': tensor([0.6855, 0.3631, 0.2749, 0.2226, 0.2065, 0.1009, 0.0944, 0.0887, 0.0545,\n",
            "        0.0519, 0.0518])}, {'boxes': tensor([[ 25.9896,  22.8188, 224.3921, 250.7727],\n",
            "        [  2.3502,  25.0676, 251.6939, 256.0000],\n",
            "        [137.0103,  20.3635, 248.1381, 236.2122],\n",
            "        [  0.0000,  17.5508, 256.0000, 256.0000],\n",
            "        [  9.8186,   9.4818, 229.9743, 251.8566],\n",
            "        [ 23.1876,  18.5769, 231.3391, 249.1120],\n",
            "        [137.4636,  22.8312, 248.2834, 233.9072],\n",
            "        [  7.2783,  10.8396, 256.0000, 256.0000],\n",
            "        [ 13.8212,  18.7071, 117.0641, 247.5923],\n",
            "        [  1.7021,  21.3092, 250.9572, 256.0000],\n",
            "        [  2.0150,  21.6675,  70.0328, 247.2096],\n",
            "        [141.2619,   5.9860, 249.7354, 245.7608],\n",
            "        [ 73.3745,   0.4782, 189.2505, 230.7792],\n",
            "        [145.2490,  11.1413, 252.5844, 255.6957],\n",
            "        [  2.5457,  17.8025, 104.9739, 256.0000]]), 'labels': tensor([17, 63, 62, 62,  1, 18, 63, 22, 63, 65, 62, 65,  1,  1,  1]), 'scores': tensor([0.5867, 0.4016, 0.3377, 0.2310, 0.2278, 0.1192, 0.1139, 0.1119, 0.1048,\n",
            "        0.0913, 0.0842, 0.0751, 0.0751, 0.0558, 0.0544])}, {'boxes': tensor([[  0.0000,   5.4600, 251.6715, 233.4359],\n",
            "        [ 35.2516,   9.4078, 191.4821, 163.2503],\n",
            "        [ 96.8004,   6.3811, 256.0000, 192.1684],\n",
            "        [  0.0000,   2.6775, 256.0000, 246.5541],\n",
            "        [  0.7414,   3.1224, 256.0000, 245.6149],\n",
            "        [ 13.0829,  14.0423, 127.8051, 173.9116],\n",
            "        [  6.5583,   0.0000, 247.9166, 247.9589],\n",
            "        [  2.1795,  20.3486, 252.3517, 249.8708],\n",
            "        [ 10.4165,  16.4700, 123.2146, 185.4816],\n",
            "        [ 14.4772,  21.5766, 163.7329, 214.6883],\n",
            "        [  0.7972, 119.9757, 254.5716, 252.7161],\n",
            "        [  4.8458,  26.3136, 253.8015, 251.0736],\n",
            "        [ 11.4372,  14.0468, 124.6872, 189.3356],\n",
            "        [ 97.2004,   2.9690, 249.1584, 247.3407]]), 'labels': tensor([ 1,  1,  1, 62, 63,  1, 17, 86, 63, 17,  1, 47, 62, 74]), 'scores': tensor([0.6419, 0.2466, 0.2462, 0.2114, 0.1931, 0.1587, 0.1557, 0.1059, 0.0915,\n",
            "        0.0678, 0.0642, 0.0568, 0.0528, 0.0506])}, {'boxes': tensor([[ 20.1915,   7.2136, 210.2149, 251.4244],\n",
            "        [ 11.9729,  12.6801, 249.9247, 256.0000],\n",
            "        [  8.4100,   4.5406, 249.0818, 254.0891],\n",
            "        [135.3287,  19.0668, 240.3356, 242.6478],\n",
            "        [118.2882,   5.7579, 245.6266, 243.2817],\n",
            "        [135.4792,   0.0000, 244.3401, 255.8049],\n",
            "        [ 11.8749,   8.4598, 248.4703, 256.0000],\n",
            "        [ 31.4404,   6.5069, 204.4001, 248.3662],\n",
            "        [127.2678,   3.7606, 247.1242, 201.3606],\n",
            "        [ 32.0279,  11.2835, 153.2397, 192.6938],\n",
            "        [ 39.7304,  22.4880, 207.1749, 252.0724],\n",
            "        [137.6313,  24.9331, 225.0480, 243.2820]]), 'labels': tensor([ 1, 63, 62,  1, 63, 62, 65, 17, 65,  1, 32, 32]), 'scores': tensor([0.5655, 0.4424, 0.2272, 0.1890, 0.1697, 0.1376, 0.1345, 0.1316, 0.1213,\n",
            "        0.0592, 0.0587, 0.0516])}, {'boxes': tensor([[100.3101,   7.4713, 250.7793, 238.0125],\n",
            "        [ 12.7922,  13.6244, 245.1382, 242.7328],\n",
            "        [  5.6542,  13.0008, 256.0000, 238.2353],\n",
            "        [  7.0074,  10.8947, 255.7425, 224.7686],\n",
            "        [137.6383,   8.7598, 244.4412, 162.8637],\n",
            "        [ 14.9702,  16.6375, 128.8212, 208.3950],\n",
            "        [126.3290,   1.9341, 253.2149, 231.6217],\n",
            "        [ 20.6972,  19.0331, 215.6627, 207.6629],\n",
            "        [  8.4698, 116.5958, 253.2475, 247.0307],\n",
            "        [  0.0000,   7.7903, 256.0000, 248.0052],\n",
            "        [ 84.0731,  32.4751, 230.5849, 172.8615],\n",
            "        [ 11.7799,  16.7551, 125.3450, 200.0589],\n",
            "        [131.7200,   4.5020, 250.1754, 207.0495],\n",
            "        [  3.9517,  22.0034,  61.6205, 225.8390],\n",
            "        [125.1103,   7.7850, 255.1678, 229.4048],\n",
            "        [ 43.6835,  28.0136, 154.6675, 167.5487],\n",
            "        [  8.9388,   6.8198, 155.5440, 177.6244],\n",
            "        [  7.5948,  14.7541, 159.5881, 214.1291]]), 'labels': tensor([ 1, 17, 63, 65,  1, 63, 63,  1,  1, 62,  1,  1, 65, 63, 62,  1, 65, 62]), 'scores': tensor([0.4297, 0.3612, 0.3590, 0.2004, 0.1910, 0.1886, 0.1762, 0.1576, 0.1445,\n",
            "        0.1345, 0.1184, 0.0831, 0.0630, 0.0606, 0.0598, 0.0563, 0.0542, 0.0501])}, {'boxes': tensor([[ 44.2062,  17.2295, 237.4935, 239.2195],\n",
            "        [  3.8841,   9.3170, 250.4767, 254.1786],\n",
            "        [  0.0000,   8.9000, 251.8665, 253.3096],\n",
            "        [ 18.8156,   8.9090, 246.1902, 247.9560],\n",
            "        [  5.6562,  12.6532, 142.9606, 204.2105],\n",
            "        [  8.0878,  12.3108, 125.1612, 215.7751],\n",
            "        [141.6300,   9.5769, 248.9973, 238.3503],\n",
            "        [  7.1741,  17.4144, 151.0236, 222.0401],\n",
            "        [ 28.0648,  16.2642, 172.6368, 188.7155],\n",
            "        [125.5445,   9.6365, 249.6303, 249.1800],\n",
            "        [123.2846,   7.8023, 250.3828, 249.5213],\n",
            "        [ 23.1821,   0.0000, 242.6940, 246.3221],\n",
            "        [  0.0000,   6.5663, 256.0000, 252.9063],\n",
            "        [ 82.0696,  23.3875, 202.0508, 188.9475]]), 'labels': tensor([17, 63, 65,  1, 63, 65,  1, 62,  1, 63, 65, 18, 62, 17]), 'scores': tensor([0.5263, 0.4485, 0.4410, 0.4050, 0.2701, 0.2014, 0.1584, 0.1330, 0.1017,\n",
            "        0.0972, 0.0956, 0.0881, 0.0698, 0.0577])}, {'boxes': tensor([[4.2893e+00, 2.5064e+01, 2.5223e+02, 2.4194e+02],\n",
            "        [1.1146e+01, 2.9405e+01, 1.3787e+02, 2.3980e+02],\n",
            "        [3.9212e+01, 2.7750e+01, 2.4953e+02, 2.3532e+02],\n",
            "        [3.1312e+01, 2.7120e+01, 2.2569e+02, 2.4439e+02],\n",
            "        [8.7137e-01, 2.2457e+01, 2.5559e+02, 2.4487e+02],\n",
            "        [1.2094e+01, 1.9228e+01, 2.1879e+02, 2.4811e+02],\n",
            "        [2.7077e+01, 2.8205e+01, 2.3156e+02, 2.4288e+02],\n",
            "        [1.1768e+01, 2.3482e+01, 1.2958e+02, 2.4892e+02],\n",
            "        [1.1116e+02, 9.7485e+00, 2.5288e+02, 2.4907e+02],\n",
            "        [2.4033e+01, 3.3503e+01, 1.1959e+02, 2.3473e+02],\n",
            "        [3.0964e+00, 1.7799e+01, 5.9663e+01, 2.4952e+02],\n",
            "        [1.4950e+02, 0.0000e+00, 2.4871e+02, 2.5337e+02],\n",
            "        [6.3448e+00, 1.3139e+01, 1.1405e+02, 2.3973e+02],\n",
            "        [1.2489e+02, 1.4117e+01, 2.5223e+02, 2.1067e+02],\n",
            "        [2.7277e+00, 7.3008e-02, 4.2626e+01, 2.4995e+02]]), 'labels': tensor([63, 63,  1, 17, 62, 65, 18, 62, 65,  1, 63, 63, 65,  1, 62]), 'scores': tensor([0.4488, 0.4314, 0.3104, 0.3057, 0.2573, 0.2283, 0.1598, 0.1456, 0.1132,\n",
            "        0.1061, 0.0761, 0.0710, 0.0660, 0.0652, 0.0517])}, {'boxes': tensor([[  0.3375,  23.3245, 256.0000, 255.5007],\n",
            "        [123.6969,  27.1350, 252.1895, 254.4048],\n",
            "        [125.2092,  25.4794, 249.2219, 250.8623],\n",
            "        [ 16.8296,  46.8226, 211.6104, 220.0871],\n",
            "        [ 36.9474,  35.9709, 185.0420, 200.1575],\n",
            "        [140.9425,  24.7628, 251.9860, 243.9655],\n",
            "        [  0.0000,  27.8723, 256.0000, 235.5677],\n",
            "        [ 11.8088,  18.5881, 247.7144, 253.4149],\n",
            "        [  0.0000,  22.7276, 256.0000, 241.6750],\n",
            "        [137.1201,  10.2274, 252.0767, 250.8735],\n",
            "        [190.1377,  28.7756, 253.4461, 251.9884],\n",
            "        [189.7388,  29.6383, 252.5793, 251.8467],\n",
            "        [141.2843,  39.0379, 224.1592, 239.9942],\n",
            "        [  0.0000,  26.4725, 128.2860, 212.0264],\n",
            "        [ 24.7924,  26.6108, 200.4416, 248.2274]]), 'labels': tensor([ 1,  1, 62, 32,  1, 65, 63, 62, 65, 63, 62, 63, 32,  1, 75]), 'scores': tensor([0.8764, 0.2921, 0.2145, 0.1565, 0.1267, 0.1237, 0.1222, 0.1086, 0.1060,\n",
            "        0.0990, 0.0797, 0.0765, 0.0725, 0.0593, 0.0526])}, {'boxes': tensor([[  2.5678,  12.1709, 247.3977, 241.1288],\n",
            "        [  1.0178,  12.0984, 249.7243, 246.6111],\n",
            "        [140.7634,  13.4123, 250.0060, 252.5638],\n",
            "        [135.3950,   0.0000, 251.1943, 248.6853],\n",
            "        [111.4799,  15.2074, 252.0621, 248.3838],\n",
            "        [ 10.8453,  16.2167, 226.9027, 249.9331],\n",
            "        [  2.4933,  23.0435,  44.0445, 238.2697],\n",
            "        [ 30.8985,   0.0000, 208.1492, 248.1270],\n",
            "        [  8.6902,   6.7479, 133.4630, 248.0613],\n",
            "        [  7.6491,  16.0865, 151.7409, 183.9030],\n",
            "        [133.5703,  25.8239, 235.1002, 243.5745],\n",
            "        [  6.0684,   5.1182, 204.4256, 252.7043],\n",
            "        [ 67.3369,   6.7772, 253.4643, 173.7686],\n",
            "        [133.7311,  15.0851, 250.0823, 249.9138]]), 'labels': tensor([63, 65, 65, 63, 62,  1, 63, 17, 63, 65,  1, 62, 65, 32]), 'scores': tensor([0.4802, 0.4353, 0.2705, 0.2405, 0.2254, 0.2098, 0.1731, 0.1697, 0.1193,\n",
            "        0.1085, 0.0959, 0.0931, 0.0928, 0.0671])}, {'boxes': tensor([[  7.6109,  18.4351, 191.3347, 255.1166],\n",
            "        [  4.9491,  20.2759, 251.9912, 248.0552],\n",
            "        [  4.2822,  10.4562, 250.3937, 249.5135],\n",
            "        [136.1965,  20.8625, 241.9804, 251.9598],\n",
            "        [127.4452,   9.8413, 252.6604, 250.0673],\n",
            "        [  4.0153,  16.8290, 244.3738, 256.0000],\n",
            "        [126.6680,  11.0411, 251.2304, 249.5686],\n",
            "        [ 14.2747,  29.9967, 116.0726, 220.8789],\n",
            "        [ 23.9699,  17.3567, 198.8678, 251.0548],\n",
            "        [  3.0618,  23.6223, 131.1648, 233.2631],\n",
            "        [  9.8606,  24.2339, 138.8139, 251.2769],\n",
            "        [ 17.2621,  17.9081, 127.2149, 246.0285],\n",
            "        [ 19.5797,  12.7947, 204.6425, 249.5294],\n",
            "        [222.7988,  36.4769, 254.3632, 252.3419],\n",
            "        [137.5526,  10.3709, 251.2595, 256.0000],\n",
            "        [113.5921,  15.4239, 251.8021, 251.8503],\n",
            "        [218.0731,  20.4081, 252.5992, 250.2087]]), 'labels': tensor([ 1, 63, 62,  1, 62, 65, 63,  1, 17, 63, 62, 65, 18, 62, 65, 22, 63]), 'scores': tensor([0.5369, 0.3550, 0.2244, 0.2084, 0.2025, 0.1909, 0.1722, 0.1606, 0.1336,\n",
            "        0.1257, 0.1188, 0.1115, 0.0769, 0.0664, 0.0602, 0.0591, 0.0525])}, {'boxes': tensor([[  4.1362,   9.6140, 227.2896, 240.0770],\n",
            "        [ 48.4784, 184.1046, 204.2026, 251.5058],\n",
            "        [  4.0691,   8.7458, 256.0000, 246.6033],\n",
            "        [ 99.8961,   7.8342, 250.1568, 250.7863],\n",
            "        [ 52.4422,  17.0335, 224.3816, 243.2432],\n",
            "        [ 11.7705,  77.9096, 256.0000, 250.6221],\n",
            "        [ 52.2354, 186.3718, 130.9526, 251.6342],\n",
            "        [131.0025,   2.2621, 247.4250, 242.3830],\n",
            "        [  3.8268,   7.5909, 250.0808, 256.0000],\n",
            "        [136.3483,   3.5714, 256.0000, 227.5213],\n",
            "        [  3.7906,  12.5709, 119.4688, 235.0295],\n",
            "        [151.9460, 191.1241, 228.8063, 246.9565],\n",
            "        [138.9334,   3.2203, 253.7200, 229.3730],\n",
            "        [ 92.2597,  24.3330, 206.0273, 211.0118],\n",
            "        [ 56.4892, 185.6989, 233.9560, 249.7278],\n",
            "        [ 35.8500,  15.6357, 227.2031, 239.7202],\n",
            "        [ 38.9109,   0.0000, 235.3583, 247.3954],\n",
            "        [  7.5172,   4.3310, 252.4158, 241.0022]]), 'labels': tensor([ 1,  1, 63, 62, 17, 62,  1, 63, 65,  1,  1, 74, 65,  1, 62, 18, 88, 22]), 'scores': tensor([0.4261, 0.4238, 0.3659, 0.2608, 0.2512, 0.1507, 0.1409, 0.1242, 0.0963,\n",
            "        0.0913, 0.0870, 0.0784, 0.0763, 0.0757, 0.0717, 0.0685, 0.0591, 0.0587])}]\n"
          ]
        }
      ]
    }
  ]
}